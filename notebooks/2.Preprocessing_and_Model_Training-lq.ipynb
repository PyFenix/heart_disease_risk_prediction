{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "# Uncomment these on first run to be able to access src\n",
    "# import sys\n",
    "# sys.path.append(\"../src\")\n",
    "from src.utils import load_or_download_data\n",
    "from src.features.preprocess_data import DataProcessor\n",
    "from src.models.train_model import ModelTrainer, HyperparameterTuner\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/lqueiros/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python312.zip', '/home/lqueiros/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12', '/home/lqueiros/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/lib-dynload', '', '/home/lqueiros/projects/ml_zoomcamp_2024_midterm_project/heart_disease_risk_prediction/.venv/lib/python3.12/site-packages', '..', '..', '..', '..', '..', '../', '../utils', '../src']\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# print(sys.path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About The Dataset :\n",
    "\n",
    "**age**: Age of the patient\n",
    "\n",
    "**sex**: Sex of the patient\n",
    " - 1 = Male\n",
    " \n",
    " - 0 = Female\n",
    "\n",
    "**cp**: Chest pain type\n",
    " - 1 = Typical Angina\n",
    "\n",
    " - 2 = Atypical Angina\n",
    "\n",
    " - 3 = Non-anginal Pain\n",
    "\n",
    " - 4 = Asymptomatic\n",
    "\n",
    "**trtbps**: Resting blood pressure (in mm Hg)\n",
    "\n",
    "**chol**: Cholestoral in mg/dl fetched via BMI sensor\n",
    "\n",
    "**fbs**: (fasting blood sugar > 120 mg/dl)\n",
    " - 1 = True\n",
    "\n",
    " - 0 = False\n",
    "\n",
    "**restecg**: Resting electrocardiographic results\n",
    " - 0 = Normal \n",
    "\n",
    " - 1 = ST-T wave normality\n",
    "\n",
    " - 2 = Left ventricular hypertrophy\n",
    "\n",
    "**thalachh**: Maximum heart rate achieved\n",
    "\n",
    "**oldpeak**: Previous peak. ST depression induced by exercise relative to rest\n",
    "\n",
    "**slp**: Slope. The slope of the peak exercise ST segment\n",
    " - 0 = unsloping\n",
    "\n",
    " - 1 = flat\n",
    "\n",
    " - 2 = downsloping\n",
    "\n",
    "**ca**: Number of major vessels ~ (0,3)\n",
    "\n",
    "**thall**: Thalassemia. Thalium Stress Test result ~ (0,3)\n",
    " - 0 = null\n",
    "\n",
    " - 1 = fixed defect\n",
    "\n",
    " - 2 = normal\n",
    "\n",
    " - 3 = reversable defect\n",
    "\n",
    "**exang**: Exercise induced angina \n",
    " - 1 = Yes\n",
    "\n",
    " - 0 = No\n",
    "\n",
    "**num**: Target variable. Diagnosis of risk for heart disease (angiographic disease status).\n",
    "\n",
    "Original values range from 0 to 4, considering the significance of 0 to be \"no risk\" and 1,2,3,4 to be \"at risk\" with the higher the number corresponding to a higher risk. However, following the example of other researchers, a simplification is performed to change the target output to binary:\n",
    "\n",
    " - 0 = < 50% diameter narrowing. less chance of heart disease\n",
    "\n",
    " - 1 = > 50% diameter narrowing. more chance of heart disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found at ../data/raw/heart_disease_original_data.csv. Loading data...\n",
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
       "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
       "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
       "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
       "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
       "\n",
       "    ca  thal  num  \n",
       "0  0.0   6.0    0  \n",
       "1  3.0   3.0    2  \n",
       "2  2.0   7.0    1  \n",
       "3  0.0   3.0    0  \n",
       "4  0.0   3.0    0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function will look for the data in the expected folder, if not present, it will download it from its source\n",
    "heart_df =  load_or_download_data()\n",
    "print(heart_df.shape)\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Columns: ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', 'num']\n",
      "Numerical Columns: ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n"
     ]
    }
   ],
   "source": [
    "# Define the numerical columns\n",
    "numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "# Define the categorical columns by excluding numerical ones\n",
    "categorical_cols = [col for col in heart_df.columns if col not in numerical_cols]\n",
    "\n",
    "print(\"Categorical Columns:\", categorical_cols)\n",
    "print(\"Numerical Columns:\", numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the data processing class\n",
    "data_processor = DataProcessor(heart_df, numerical_cols, categorical_cols, 'num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing values\n",
    "original_number_of_rows = heart_df.shape[0]\n",
    "heart_df_cleaned = data_processor.clean_data().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 6 rows dropped due to containing missing data\n"
     ]
    }
   ],
   "source": [
    "rows_dropped = original_number_of_rows - heart_df_cleaned.shape[0]\n",
    "print(f\"There were {rows_dropped} rows dropped due to containing missing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarizing the target and changing the categorical features data type\n",
    "heart_df_processed = data_processor.preprocess_data().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 297 entries, 0 to 301\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   age       297 non-null    int64   \n",
      " 1   sex       297 non-null    category\n",
      " 2   cp        297 non-null    category\n",
      " 3   trestbps  297 non-null    int64   \n",
      " 4   chol      297 non-null    int64   \n",
      " 5   fbs       297 non-null    category\n",
      " 6   restecg   297 non-null    category\n",
      " 7   thalach   297 non-null    int64   \n",
      " 8   exang     297 non-null    category\n",
      " 9   oldpeak   297 non-null    float64 \n",
      " 10  slope     297 non-null    category\n",
      " 11  ca        297 non-null    category\n",
      " 12  thal      297 non-null    category\n",
      " 13  num       297 non-null    category\n",
      "dtypes: category(9), float64(1), int64(4)\n",
      "memory usage: 17.8 KB\n"
     ]
    }
   ],
   "source": [
    "heart_df_processed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (177, 13), Validation set: (60, 13), Test set: (60, 13)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = data_processor.split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models in a dictionary\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Support Vector Classifier': SVC(),\n",
    "    'XGBoost Classifier': XGBClassifier(eval_metric=\"logloss\", enable_categorical=True),\n",
    "    'Decision Tree': DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Training Support Vector Classifier...\n",
      "Training XGBoost Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lqueiros/projects/ml_zoomcamp_2024_midterm_project/heart_disease_risk_prediction/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree...\n",
      "************\n",
      "All models trained! Next step is performance evaluation.\n",
      "************\n",
      "************\n",
      "Evaluating Logistic Regression...\n",
      "************\n",
      "Confusion Matrix for Logistic Regression: \n",
      "[[28  2]\n",
      " [ 6 24]]\n",
      "Accuracy of Logistic Regression: 86.67%\n",
      "Precision of Logistic Regression: 92.31%\n",
      "Recall of Logistic Regression: 80.00%\n",
      "F1 Score of Logistic Regression: 85.71%\n",
      "\n",
      "============================================================\n",
      "\n",
      "Evaluating Support Vector Classifier...\n",
      "************\n",
      "Confusion Matrix for Support Vector Classifier: \n",
      "[[25  5]\n",
      " [18 12]]\n",
      "Accuracy of Support Vector Classifier: 61.67%\n",
      "Precision of Support Vector Classifier: 70.59%\n",
      "Recall of Support Vector Classifier: 40.00%\n",
      "F1 Score of Support Vector Classifier: 51.06%\n",
      "\n",
      "============================================================\n",
      "\n",
      "Evaluating XGBoost Classifier...\n",
      "************\n",
      "Confusion Matrix for XGBoost Classifier: \n",
      "[[25  5]\n",
      " [ 7 23]]\n",
      "Accuracy of XGBoost Classifier: 80.00%\n",
      "Precision of XGBoost Classifier: 82.14%\n",
      "Recall of XGBoost Classifier: 76.67%\n",
      "F1 Score of XGBoost Classifier: 79.31%\n",
      "\n",
      "============================================================\n",
      "\n",
      "Evaluating Decision Tree...\n",
      "************\n",
      "Confusion Matrix for Decision Tree: \n",
      "[[22  8]\n",
      " [11 19]]\n",
      "Accuracy of Decision Tree: 68.33%\n",
      "Precision of Decision Tree: 70.37%\n",
      "Recall of Decision Tree: 63.33%\n",
      "F1 Score of Decision Tree: 66.67%\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the model trainer class\n",
    "ModelTrainer(models).train_models(X_train=X_train, y_train=y_train).evaluate_models(X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the parameter grid\n",
    "model_param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"penalty\": [\"l1\", \"l2\"],\n",
    "            \"C\": [0.01, 0.1, 1, 5, 10],\n",
    "            \"solver\": [\"liblinear\"],# \"saga\"],\n",
    "        },\n",
    "    },\n",
    "    \"Support Vector Classifier\": {\n",
    "        \"model\": SVC(),\n",
    "        \"params\": {\n",
    "            \"C\": [0.01, 0.1, 1, 10],\n",
    "            \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "            \"gamma\": [\"scale\", \"auto\"],\n",
    "        },\n",
    "    },\n",
    "    \"XGBoost Classifier\": {\n",
    "    \"model\": XGBClassifier(eval_metric=\"logloss\", enable_categorical=True),\n",
    "    \"params\": {\n",
    "        \"learning_rate\": [0.02, 0.05, 0.1],  # Smaller learning rates for better generalization\n",
    "        \"n_estimators\": [50, 100],    # Number of boosting rounds\n",
    "        \"max_depth\": [3, 5, 10],      # Limit depth to control model complexity\n",
    "        \"min_child_weight\": [1, 3],   # Minimum sum of instance weights in a child\n",
    "        \"gamma\": [0, 0.1, 0.3],       # Minimum loss reduction to make a split\n",
    "        \"subsample\": [0.8, 1.0],      # Fraction of samples for each tree\n",
    "        \"colsample_bytree\": [0.8, 1.0],  # Fraction of features for each tree\n",
    "        \"reg_alpha\": [0, 0.1, 1.0],   # L1 regularization\n",
    "        \"reg_lambda\": [1.0, 2.0],     # L2 regularization\n",
    "        },\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "        \"params\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": [5, 10, 20],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 5],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the hyperparameter tuner class\n",
    "model_tuner = HyperparameterTuner(model_param_grids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring metrics\n",
    "\n",
    "Because this is a medical problematic, the best score to tune the models to is recall, in order to reduce the number of false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparameters for Logistic Regression...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Logistic Regression: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best Recall Score for Logistic Regression: 0.772\n",
      "\n",
      "Tuning hyperparameters for Support Vector Classifier...\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best Parameters for Support Vector Classifier: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best Recall Score for Support Vector Classifier: 0.736\n",
      "\n",
      "Tuning hyperparameters for XGBoost Classifier...\n",
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n",
      "Best Parameters for XGBoost Classifier: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 50, 'reg_alpha': 0, 'reg_lambda': 1.0, 'subsample': 0.8}\n",
      "Best Recall Score for XGBoost Classifier: 0.784\n",
      "\n",
      "Tuning hyperparameters for Decision Tree...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best Parameters for Decision Tree: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Recall Score for Decision Tree: 0.758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tuning the models and retrieving the best performant ones on recall\n",
    "best_models = model_tuner.tune_models(X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': {'best_estimator': LogisticRegression(C=10, penalty='l1', solver='liblinear'),\n",
       "  'best_params': {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'},\n",
       "  'best_score': np.float64(0.7720588235294118)},\n",
       " 'Support Vector Classifier': {'best_estimator': SVC(C=0.1, kernel='linear'),\n",
       "  'best_params': {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'},\n",
       "  'best_score': np.float64(0.7360294117647059)},\n",
       " 'XGBoost Classifier': {'best_estimator': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=True, eval_metric='logloss',\n",
       "                feature_types=None, gamma=0, grow_policy=None,\n",
       "                importance_type=None, interaction_constraints=None,\n",
       "                learning_rate=0.05, max_bin=None, max_cat_threshold=None,\n",
       "                max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
       "                max_leaves=None, min_child_weight=3, missing=nan,\n",
       "                monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "                n_jobs=None, num_parallel_tree=None, random_state=None, ...),\n",
       "  'best_params': {'colsample_bytree': 0.8,\n",
       "   'gamma': 0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 0,\n",
       "   'reg_lambda': 1.0,\n",
       "   'subsample': 0.8},\n",
       "  'best_score': np.float64(0.7838235294117647)},\n",
       " 'Decision Tree': {'best_estimator': DecisionTreeClassifier(criterion='entropy', max_depth=20),\n",
       "  'best_params': {'criterion': 'entropy',\n",
       "   'max_depth': 20,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2},\n",
       "  'best_score': np.float64(0.7580882352941176)}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking into the trained models\n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: XGBoost Classifier\n",
      "Best Estimator: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric='logloss',\n",
      "              feature_types=None, gamma=0, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
      "              max_leaves=None, min_child_weight=3, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)\n",
      "Best Params: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 50, 'reg_alpha': 0, 'reg_lambda': 1.0, 'subsample': 0.8}\n",
      "Best Score: 0.7838235294117647\n"
     ]
    }
   ],
   "source": [
    "# Selecting the best performing model\n",
    "best_model = model_tuner.get_best_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing model after tuning is XGBoost Classifier\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best performing model after tuning is {best_model.best_model[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ../models/trained_model.pkl\n"
     ]
    }
   ],
   "source": [
    "model_tuner.save_best_model_to_pickle(filepath=\"../models/trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model for validation purposes only\n",
    "with open(\"../models/trained_model.pkl\", \"rb\") as f:\n",
    "    best_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "predictions = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on validation set: 0.833\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: Calculate recall\n",
    "recall = recall_score(y_val, predictions)\n",
    "print(f\"Recall on validation set: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
